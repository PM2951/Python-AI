[戻る](https://github.com/PM2951/Python-forMAC)    

# ダイヤモンドで学ぶ機械学習

このノートでは、ダイヤモンドのカラットと価格データを使って、以下の流れで機械学習を学びます。  
適宜googleで用法を調べてください。

### 目的:  ダイヤモンドのカラット情報を基に、ダイヤモンドの販売価格を予測するAIモデルをつくる

目次
1. 問題設定と全体の考え方
2. データの読み込みと概要確認
3. 相関の確認（可視化と相関係数）
4. データ整形・前処理
5. データ分割（8:2 の train/test）
6. 単回帰（carat → price）
7. 重回帰（複数特徴量 → price）
8. LightGBM によるモデル化
9. まとめと発展

---

## 0. 事前準備（環境とライブラリ）

使用する主なライブラリ：

* pandas, numpy（データ処理）
* matplotlib, seaborn（可視化）
* scikit-learn（回帰モデル・データ分割・評価指標）
* lightgbm（勾配ブースティング木）

```bash
pip install pandas numpy matplotlib seaborn scikit-learn lightgbm
```

Python スクリプトや Jupyter Notebook で次をインポートする：

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

from lightgbm import LGBMRegressor
```

---

## 1. 問題設定と考え方


最初に「何を予測したいのか」「どのようなデータを使えるのか」をはっきりさせておくと、以降の前処理・モデル選択・評価指標の選択がぶれにくくなります。価格予測なのか、ランク分類なのか、あるいは両方なのかを曖昧にしたまま進めると、途中で「そもそも何をしたかったのか」が分からなくなり、コードもノートも読みづらくなります。  
特に中級者以降は、実装よりも「問題設定の妥当性」がボトルネックになりがちなので、ここで少し時間をかけて整理しておくことが重要です。

今回は、「ダイヤモンドの直径や深さ、幅、色、カラット数、研磨の精度、透明度、などの情報から、そのダイヤモンドの販売価格を予測する」という課題です。

### 用語

* **目的変数（target / y）**: 予測したいデータ。今回はダイヤモンド価格 `price`。
* **説明変数（features / X）**: 今回は深さ、幅、色、カラット数など、目的変数に設定したデータの予測に使う情報。
* **回帰問題（regression）**: 連続値（実数など）を予測するタスク。
* **分類問題（classification）**: 離散的カテゴリ（低, 中, 高やA, B, Cなどの段階的な変化をするもの）を予測するタスク。

「説明変数 と 目的変数 をどう定義するか」「回帰として扱うのか、分類として扱うのか」という問題設定そのものを整理します。  
回帰問題か分類問題かで解析手法がやや異なってきます。

---

## 2. データの読み込みと概要確認

データを読み込んだらすぐにモデルを回したくなりますが、まずは「どんな列があり、どのくらいの行数があり、欠損や異常値はないか」をざっくり把握することが重要です。ここを怠ると、

* 想定と違う単位（例: カラットではなくグラムだった）
* 目的変数が極端に偏っている
* そもそも使えない特徴量が多い

といった問題に後から気づき、やり直しになることが多いです。

`head()`, `info()`, `describe()` などは最初に「必ず通すチェックポイント」として習慣化するとよいです。  

**解説**:  
* `df.info()`: 各列のデータ型（int, float, object など）、欠損値の有無、行数を確認する。
* `df.describe()`: 数値列の統計量（平均、標準偏差、最小値、四分位数、最大値）を確認する。分布のスケール感や外れ値の有無の目安を得る。

### 用語

* **DataFrame (`pd.DataFrame`)**: 行×列の 2 次元表形式でデータを保持する pandas の基本構造。
* **`sns.load_dataset("diamonds")`**: seaborn に同梱されているサンプルデータセットを DataFrame として読み込む関数。
* **`df.head()`**: 先頭数行を表示し、カラム名や値の雰囲気を確認。
* **`df.info()`**: 列ごとのデータ型や欠損数を確認。
* **`df.describe()`**: 数値列の基本統計量を確認。

この章では、「ダイヤモンド価格データを DataFrame として読み込み、ざっくり中身を把握する」ことを目的とします。

### 練習問題

1. `sns.load_dataset("diamonds")`でデータをDataFrameで読み込み `df.info()` と `df.describe()` でデータの内容を確認してください
2. `diamonds` データセットの行数と列数を取得するコードを書いてください。  
3. `df["cut"].value_counts()` を用いて、カットグレードの分布をグラフにし、「どのグレードが最も多いか」「不均衡がありそうか」を分析してください。
4. `df["price"].describe()` の結果をもとに、価格の中央値・平均・最大値を表示し、「この分布にどのような特徴（右に長い尾など）がありそうか」を分析してください。

---

## 3. 相関の確認

相関の確認は、

* どの特徴量が目的変数に強く効きそうか
* 特徴量同士が強く相関していて、多重共線性の問題を起こしそうか

といった「特徴量設計・モデル選択」の出発点になります。単回帰では相関の高い特徴量から試すのが自然です。  
例えば、通学にかかる時間は「家から学校までの距離」や「平均移動速度」に非常に相関しそうで、「信号機の数」や「高低差」みたいな情報はあまり相関がなさそう、のような情報が得られます。

また、「相関が高すぎる特徴量を同時に入れない」などの判断材料になります。  
加えて、散布図を描くことで「線形関係か、非線形か」「外れ値がどの程度あるか」を視覚的に確認できます。

### 用語

* **散布図 (`plt.scatter`)**: 2 変数間の関係を点のプロットで可視化。
* **ピアソン相関係数**: 2 つの連続変数の「線形な関係の強さ」を -1〜1 の値で表す指標。
* **相関行列 (`df.corr()`)**: 複数の数値列同士の相関係数をまとめた行列。
* **ヒートマップ (`sns.heatmap`)**: 相関行列を色で可視化する図。

ここでは、「carat と price の関係」「他の数値特徴量と price の関係」を定量的・視覚的に把握します。

### 練習問題

1. `carat` と `price` のピアソン相関係数を計算するコードを書いてください。
2. dfの各データ間の相関係数をヒートマップで可視化し、`price` との相関が高い上位 3 つの特徴量を分析してください。


---

## 4. データ整形・前処理

前処理は、モデルの性能だけでなく「再現性」と「解釈性」に直結します。

* 欠損や外れ値を放置すると、学習が不安定・過学習・変な予測につながる。
* カテゴリ変数の扱いを適当にすると、後から結果を再現できなくなる。
* どのようなフィルタリングをしたかを明示しておくことで、他人がノートを読んだときに安心して結果を信頼できる。

「なぜその処理をしたか」をコメントで残しておくことが特に重要になります。

### 用語

* **欠損値（NaN）**: データが存在しないセル。`df.isna().sum()` で列ごとに数を確認。
* **外れ値（outlier）**: 全体の分布から大きく外れた極端な値。箱ひげ図などでざっくり確認。
* **分位点（quantile）によるフィルタリング**: 上位 1% などを切り落として極端な外れ値を除去できる。
* **カテゴリ変数の one-hot エンコーディング (`pd.get_dummies`)**: {"Fair", "Good", ...} のようなラベルを 0/1 の複数列に展開する処理。文字列などを0,1,2...という数値に変換することで四則演算が可能になる。

ここでは、「学習にかけやすい形にデータを整える」ことが主な目的です。

### 練習問題

1. 価格の上位 1% を除外するフィルタリングをしてください。
2. dfのカテゴリ変数を one-hot エンコーディングしてください。 `drop_first=True` を引数に用いた際の結果と比較してください。
3. `df_filtered` の `carat` と `price` の散布図を再度描き、フィルタリング前後で外れ値の程度がどう変化したかを分析してください。
4. `cut` の rare なカテゴリ（出現頻度が非常に少ないカテゴリ）が存在すると仮定し、それをまとめて `"Other"` カテゴリに統合してください

---

## 5. データ分割（8:2）


1 つのデータだけで学習と評価を同時に行うと、モデルがそのデータに「答えを暗記」してしまい、未知データに対する性能（汎化性能）が全く分からなくなります。そこで、

* 学習用（train）
* 評価用（test）

にデータを分け、評価用データには「一切触れずに予測モデルを作成し」最後に評価用データでモデルの予測精度を検証する、という流れが重要です。8:2 はその一例で、データ量やタスクによって割合は調整します。

### 用語

* **`train_test_split`**: scikit-learn の関数。`X` と `y` を指定した比率で学習用とテスト用に分割する。
* **`test_size` / `train_size`**: テストデータ（またはトレーニングデータ）の割合や件数を指定する引数。
* **`random_state`**: 乱数シード。分割結果を再現可能にするために固定する。
* **`X_train`, `X_test`, `y_train`, `y_test`**: 分割後の説明変数・目的変数のセット。

**解説（例）**:

```python
from sklearn.model_selection import train_test_split

X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(
    X_single, y, test_size=0.2, random_state=42
)
print(len(X_train_s), len(X_test_s))
```
=>  `test_size=0.2`とすることで、全体の2割が評価用（test）データに分割されます。  
=>  `random_state` を使用することで分割結果が毎回同じになり、実験の再現性が確保されます。別環境や別の日に同じコードを実行しても同じ train/test の分割になる。

この章では、以降すべてのモデルで共通して使う「評価の土台」を作ります。

### 練習問題

1. `train_test_split` を用いて、dfの80% を学習、20% をテストとするコードを書き、`X_train_s`, `X_test_s` の行数が想定どおりになっているか確認してください。

---

## 6. 単回帰：carat → price

単回帰は、

* 「最も単純なベースラインモデル」を持つ
* モデルの振る舞いを数式で完全に把握できる

という大きな利点があります。より複雑なモデル（重回帰や LightGBM）に進む前に、単回帰で「どの程度までシンプルな線形関係で価格が説明できるか」を把握しておくと、後のモデル比較がしやすくなります。

### 用語

* **単回帰（simple linear regression）**: 1 つの説明変数 x と目的変数 y の線形関係  
  _y = β0 + β1*x_ を学習するモデル。いわゆる一次関数
* **`LinearRegression`（scikit-learn）**: 最小二乗法に基づく線形回帰モデルのクラス。
* **`fit(X, y)`**: 学習用データから係数・切片を推定するメソッド。
* **`predict(X)`**: 学習済みモデルを使って新しいデータの予測値を計算するメソッド。
* **決定係数 (R^2)**: 回帰分析モデルの当てはまりの良さを示す指標で「寄与率」とも呼ばれます。目的変数の分散のうち、モデルが説明できる割合を示し、0から1の値をとります。値が1に近いほど回帰式の精度が高く、モデルの説明力が優れていることを意味します。
* **二乗平均平方誤差(RMSE)**: 予測値と実際の値との差（誤差）を二乗したもの（二乗誤差）を、データ点数で割ったもの（二乗平均誤差）の平方根を取ったものです。

実装例は次のようになります:

```python
from sklearn.linear_model import LinearRegression

# 説明変数xと目的変数の設定
X = df[["carat"]]
y = df["price"]

#　データをtrainとtestに分割
X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X, y, test_size=0.2, random_state=42)

# 線形回帰モデルの導入
model = LinearRegression()
# 線形回帰モデルの学習
model.fit(X_train_s, y_train_s)
# 線形回帰モデルの予測
y_pred = model.predict(X_test_s)

# 回帰係数
print(model.coef_)
# 切片
print(model.intercept_)
# 決定係数
print(model.score(X, Y))

# 二乗平均平方誤差(RMSE)
from sklearn.metrics import mean_squared_error
rmse = mean_squared_error(y_test_s, y_pred)
print(f"RMSE: {round(rmse, 3)}")
```

### 練習問題

1. 上述のコードを参考に、学習データを使って線形回帰モデルを学習し、評価用データの価格の予測値（y_pred）を算出してください
2. 1で求めた予測値（y_pred）と実際の値（y_test_s）とのブレを散布図で可視化してください。（完全に当たっていれば x=y の直線上に乗る）
3. 予測値（y_pred）と実際の値（y_test_s）間のRMSEと決定係数R^2を求めてください。

---

## 7. 重回帰：複数特徴量 → price

現実の問題では、1 つの特徴量だけで目的変数を説明しきれることはほとんどありません。そこで、重回帰により、

* 複数の数値特徴量
* カテゴリ特徴量（cut, color, clarity など）

を同時に扱うことで、より実際の価格決定に近いモデルを構築できます。また、係数を眺めることで「どの要因が価格にどの程度効いているか」を線形モデルの枠内で解釈できます。

### 用語

* **重回帰（multiple linear regression）**: 複数の説明変数を一度に扱う線形回帰。  
  _y = β0 + β1*x1 + β2*x2 + ..._ を学習するモデル。いわゆる多変数関数
* **設計行列（design matrix）**: one-hot されたカテゴリ変数を含むデータ。
* **多重共線性（multicollinearity）**: 説明変数同士が強く相関している状態。係数の不安定さや解釈の難しさにつながる。多変数関数は、各変数が完全に独立しているほうが望ましい
* **係数テーブル (`coef_`)**: 重回帰モデルから得た係数をまとめたもの。（ β0, β1, β2, ...）

典型的なコードの流れは次の通りです:

```python
#数値データ列
features_numeric = ["carat", "depth", "table", "x", "y", "z"]
#カテゴリデータ列
features_categorical = ["cut", "color", "clarity"]

#説明変数と目的変数の設定
X = df[features_numeric + features_categorical]
y = df["price"]

#カテゴリデータをone-hotエンコーディング
X_encoded = pd.get_dummies(X, columns=features_categorical, drop_first=True)

#データの分割
X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y, test_size=0.2, random_state=42
)


model = LinearRegression()
model.fit(X_train, y_train)

print(model.coef_)  #回帰変数の表示
print(model.intercept_)  #回帰直線の切片
print(model.get_params())  #パラメータの取得

y_pred = model.predict(test_x))  #予測値
print(model.score(test_x,test_y))  #決定係数の表示
```

### 練習問題

1. 上述のコードを参考に、学習データを使って線形回帰モデルを学習し、評価用データの価格の予測値（y_pred）を算出してください
2. 1で求めた予測値（y_pred）と実際の値（y_test_s）とのブレを散布図で可視化してください。（完全に当たっていれば x=y の直線上に乗る）
3. RMSEと決定係数R^2を求めてください。
4. `coef_` をDataFrameに格納し、上位 20 行をもとに、「価格を上げる方向に効いている特徴量」「下げる方向に効いている特徴量」を分析してください。

---

## 8. LightGBM によるモデル化


LightGBM のような**勾配ブースティング決定木モデル**は、

* 非線形な関係
* 特徴量同士の複雑な相互作用

を自動的に捉えやすく、実務でも非常に広く使われています。線形モデルと比べてブラックボックス度は上がりますが、その分表現力も大きく向上するため、「線形モデルでは頭打ちになった性能をどこまで伸ばせるか」を試す段階として重要です。

- メリット  
  線形回帰は特徴量のスケールに敏感であり、正則化を入れる場合などは標準化がほぼ必須になる。一方、決定木ベースのモデルは「しきい値による分割」で特徴量を扱うため、スケールの違いの影響を受けにくく、標準化をしなくても動作することが多い。

### 用語

* **決定木（decision tree）**: 「ある特徴量がしきい値より大きいかどうか」で分割を繰り返し、予測値を出す木構造のモデル。
* **勾配ブースティング（gradient boosting）**: 弱い決定木を多数足し合わせて強力なモデルを作る手法。前の木の誤差を次の木が補正していくイメージ。
* **LightGBM**: Microsoft によって開発された高速・高性能な勾配ブースティング決定木ライブラリ。大規模データや高次元データに強い。
* **`LGBMRegressor`**: LightGBM による回帰モデル。

最小限の回帰タスク用コード例は次のようになります:

```python
from lightgbm import LGBMRegressor

# すでに one-hot 済みの X_encoded, y があるとする
X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y, test_size=0.2, random_state=42
)

model = LGBMRegressor(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=-1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1,
)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
```

### 練習問題

1. `n_estimators`や `learning_rate` などをいくつか組み合わせて変化させ、テストデータの RMSE と R^2 の最適解を求めてみてください。
2. 説明変数を独自に変化、追加し、高精度なモデルを作成してください。


---

## 9. 発展

このチュートリアルで行ったこと：

1. **EDA**：散布図・相関係数で carat と price の関係を確認。
2. **前処理**：カテゴリ変数のダミー化、必要に応じて外れ値処理。
3. **データ分割**：8:2 の train/test 分割で汎化性能を評価する枠組みを作成。
4. **単回帰**：carat のみを使ったベースラインモデル。
5. **重回帰**：複数特徴量・カテゴリ変数を取り入れた線形モデル。
6. **LightGBM**：非線形モデルで性能をさらに高める。

### 発展的なテーマ

* 標準化・正則化（Ridge, Lasso）を用いた線形回帰
* クロスバリデーション（`cross_val_score`, `GridSearchCV`, `RandomizedSearchCV`）によるハイパーパラメータ探索
* 価格の対数変換（`log(price)`）を目的変数にしてみる
* 評価指標として MAE（平均絶対誤差）を追加
* LightGBM のハイパーパラメータチューニング

---
